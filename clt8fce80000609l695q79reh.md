---
title: "1 of 4: Scraping Website URLs."
datePublished: Fri Mar 01 2024 09:00:35 GMT+0000 (Coordinated Universal Time)
cuid: clt8fce80000609l695q79reh
slug: 1-of-4-scraping-website-urls
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1709280696780/d4402d03-681e-4cdc-9e41-39ed3ecd3fe1.png
tags: nodejs, mariadb, lxc, containerization, axios, web-scraping, nvm, node-js-development, cheerio, lxd, linux-containers, web-scraping-tutorial, coding-experiments, url-scraper, javascript-scraping

---

# TL;DR.

This post is a guide to setting up a Linux Container (LXC) on Ubuntu. Within the container, I will run a Node.js script for scraping URLs. This piece covers the installation of LXD (LinuX Daemon), creating and configuring an LXC, and using NVM (Node Version Manager) to install Node.js and NPM. My tutorial also delves into web scraping with Axios and Cheerio, trawling the MariaDB website as a practical example. This article underscores the benefits of using containers for experimental development and isolated deployment, emphasizing the efficiency and flexibility it offers to coders.

> **Attributions:**
> 
> [https://ubuntu.com/tutorials/how-to-run-docker-inside-lxd-containers](https://ubuntu.com/tutorials/how-to-run-docker-inside-lxd-containers)***↗.***

# An Introduction.

In this post, I show how to install the LinuX Daemon (LXD) on Ubuntu and set up a LinuX Container (LXC) for the purpose of running a Node.js script designed for URL scraping.

> The purpose of this post is to demonstrate the utility of LXCs by building a URL Scraper project.

# The Big Picture.

I use a number of isolation technologies including [LXC/LXD](https://solodev.app/installing-lxd-and-using-lxcs), [Distrobox](https://solodev.app/installing-distrobox), [Docker](https://solodev.app/installing-docker), and [Miniconda](https://solodev.app/installing-miniconda). I've already written about [the magic of VMs and containers](https://solodev.app/the-magic-of-vms-and-containers) and I find isolation technologies particularly useful for app deployment and coding experiments. Every developer needs to adopt isolation technologies for the sake of maintaining a stable host distro.

Also, I use an imaging solution called [CloneZilla](https://clonezilla.org/) that backs up my host system.

The faster I recover from disaster, the quicker I can get back to coding.

# Prerequisites.

* A Debian-based Linux distro (I use Ubuntu).
    

# Updating the System.

* From the host terminal, I update my system:
    

```bash
sudo apt clean && \
sudo apt update && \
sudo apt dist-upgrade -y && \
sudo apt --fix-broken install && \
sudo apt autoclean && \
sudo apt autoremove -y
```

# What is LXD and LXC?

LXD (LinuX Daemon) is a container manager for creating and managing LXCs (LinuX Containers.) As a background service, LXD can automatically start containers when the host system boots.

LXCs (LinuX Containers) are isolated, OS-level virtualizations which, for efficiency, uses the Linux kernel of the host system. LXCs are virtual environments where its system processes can *not* affect other containers, or the host system, without running specific commands.

I ensure that [LXD is installed](https://solodev.app/installing-lxd-and-using-lxcs) (`lxc --version`) before continuing with this post.

## Setting Up an LXC.

* I list the existing containers:
    

```bash
lxc ls
```

> NOTE: If this command fails, then [I will install LXD](https://solodev.app/installing-lxd-and-using-lxcs) before continuing with this post.

* I launch a new container called Scrapings:
    

```bash
lxc launch ubuntu:22.04 Scrapings
```

* I bash into the container:
    

```bash
lxc exec Scrapings -- bash
```

* I update and upgrade the container:
    

```bash
sudo apt clean && \
sudo apt update && \
sudo apt dist-upgrade -y && \
sudo apt --fix-broken install && \
sudo apt autoclean && \
sudo apt autoremove -y
```

## Adding a User Account to the LXC.

* From the container terminal, I add a new user:
    

```bash
adduser yt
```

* I add the new user to the `sudo` group:
    

```bash
usermod -aG sudo yt
```

> NOTE: usermod let's me (-a)ppend the sudo (-G)roup to the yt account.

* I exit the container:
    

```bash
exit
```

> NOTE: I exit the `root` account to login to the `yt` account.

## Setting the LXC Home Directory.

* From the host terminal, I login to the container with the `yt` account:
    

```bash
lxc exec Scrapings -- su yt
```

> NOTE: At the moment, the home directory is `/root`. This section will address the issue by changing the home directory to `~`.

* I use the Nano text editor to open the `.bashrc` file:
    

```bash
sudo nano ~/.bashrc
```

* I copy the following, add it (CTRL + SHIFT + V) to the bottom (CTRL + END) of the `.bashrc` file, save (CTRL + S) the changes, and exit (CTRL + X) Nano:
    

```bash
cd ~
```

## Hardening the LXC.

* From the container terminal, I use the `Nano` text editor to open the `sshd_config` file:
    

```bash
sudo nano /etc/ssh/sshd_config
```

* I copy the following, add it (CTRL + SHIFT + V) to the bottom (CTRL + END) of the `sshd_config` file, save (CTRL + S) the changes, and exit (CTRL + X) Nano:
    

```bash
PasswordAuthentication no
PermitRootLogin no
Protocol 2
Port = 2424
```

* I `restart` the `SSH` service:
    

```bash
sudo systemctl restart ssh
```

* I reboot the container:
    

```bash
sudo reboot
```

> NOTE: Within the container, I can also install (or enable) [UFW](https://solodev.app/creating-a-local-linux-container#heading-optional-enabling-and-setting-up-ufw), [Fail2Ban](https://solodev.app/creating-a-local-linux-container#heading-optional-installing-and-setting-up-fail2ban), and [CrowdSec](https://solodev.app/10-of-10-crowdsec-in-the-docker-container).

# What is Node.js, NPM, and NVM?

Node.js is a free, JavaScript (JS) server runtime. It runs JS code as single-threaded, non-blocking, asynchronous programs, which makes it very memory efficient. Node.js performs many system-level tasks, but is commonly used to run JS servers.

NPM (Node Package Manager) is the world's largest software registry. Open source developers use it to share packages with each other. Many organizations use NPM to manage private development as well. Most developers interact with NPM using the CLI (Command Line Interface), and it usually ships with Node.js.

NVM stands for Node.js Version Manager. It is used to switch between versions of Node.js. NVM works on any POSIX-compliant shell (sh, dash, ksh, zsh, bash) and runs on Linux distros, macOS, and Windows WSL.

[https://nodejs.org/en/learn/getting-started/introduction-to-nodejs](https://nodejs.org/en/learn/getting-started/introduction-to-nodejs)↗,

[https://docs.npmjs.com/about-npm](https://docs.npmjs.com/about-npm)↗, and

[https://github.com/nvm-sh/nvm#intro](https://github.com/nvm-sh/nvm#intro)↗.

I ensure that [Node.js is installed](https://solodev.app/installing-node-and-npm-with-nvm) (`node -v`) before continuing with this post.

## Testing for Node.

* I confirm the Node installation:
    

```bash
node -v
```

> NOTE: If this command fails, then I will install [Node.js and NPM with NVM](https://solodev.app/installing-node-and-npm-with-nvm) before continuing with this post.

# What are URL Scrapings?

URLs (uniform resource locations) describe where specific assets (webpages, email services, downloads, etc.) are found. These are usually fed into the address bar of my browser either manually or via a link on a webpage. URL scraping is the process of finding all the URLs at a specified, domain-name based address and then saving the results to a file or database.

## Creating the Scraper File.

* From the container terminal, I use NPM to install Axios and Cheerio:
    

```bash
npm install axios cheerio
```

* I use the Nano text editor to create the `scraper.js` JavaScript file:
    

```bash
sudo nano scraper.js
```

* I copy the following, paste (CTRL + SHIFT + V) it into the file, save (CTRL + S) the changes, and exit (CTRL + X) Nano:
    

```javascript
const fs = require('fs');
const axios = require('axios');
const cheerio = require('cheerio');

const scrapeLinks = async (url) => {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  let counter = 0;

  $('a').each((index, element) => {
    setTimeout(() => {
      const link = $(element).attr('href');
      fs.appendFile('scraped.txt', 'https://mariadb.com/kb/en' + link + '\n', (err) => {
        if (err) throw err;
        counter++;
        console.log(`Link number ${counter} saved!`);
      });
    }, index * 1000);
  });
};

scrapeLinks('https://mariadb.com/kb/en');
```

> NOTE 1: There is a 1 second (1,000ms) pause between each request.

> NOTE 2: The MariaDB website is used as an example.

## Running the Scraper.

* From the container terminal, I run `scraper.js`:
    

```bash
node scraper.js
```

## Collecting the Results.

* When `scraper.js` completes, I clear the container terminal:
    

```bash
clear
```

* I display the results from `scraped.txt` in the container terminal:
    

```bash
cat scraped.txt
```

* I use my mouse to select all the displayed links, and then CTRL + SHIFT + C to save everything to the clipboard.
    
* In a desktop editor like VS Code, I paste (CTRL + V) the results.
    

# The Results.

To explore URL scraping, I started with setting up an LXC on Ubuntu, installing Node.js and NPM using NVM, writing a scraper script in JavaScript, and using Node.js to scraping websites. Using Axios and Cheerio for scraping demonstrates the power and simplicity of using Node.js for web scraping tasks. This guide serves as a technical walk-through and also highlights the versatility of using containers for development tasks. It is evident that LXC containers offer a robust platform for developing and deploying many kinds of solutions, opening up a world of safe experimentation.

# In Conclusion.

In the realm of web development, efficiency and isolation are key. This is where Linux Containers (LXC) offers a lightweight alternative to traditional VMs. I've explored URL scraping within an LXC, leveraging the power of Node.js.

Why LXC? It provides an isolated environment for running applications, ensuring that your scraping activities don't interfere with your system or other containers.

Set up was a breeze:

* Starting with a fresh LXC on Ubuntu, I ventured into the installation of Node.js and NPM via NVM.
    
* The magic happened when I combined Axios and Cheerio for efficient web scraping, all running smoothly within my container.
    

The result? A streamlined process that not only simplifies development tasks but also opens up a sandbox for safe experimentation. Containers like LXC are invaluable tools for developers, offering a blend of flexibility, efficiency, and security.

Have you tried running development tasks within a container? What has your experience been like?

Let's discuss in the comments below!

Until next time: Be safe, be kind, be awesome.

#WebScraping #WebScrapingTutorial #NodeJs  
#NodeJsDevelopment #LinuxContainers #Containerization  
#LXD #LXC #NVM #Axios #Cheerio #MariaDB  
#CodingExperiments #URLScraper #JavaScriptScraping  
#TechGuide #Development #DevelopmentIsolation  
#SecureCoding #TechTalk #TechInnovation  
#ContainerizedDevelopment #Ubuntu