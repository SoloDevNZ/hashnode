---
title: "URL Scrapings, in an LXC, with Node."
datePublished: Fri Mar 01 2024 09:00:35 GMT+0000 (Coordinated Universal Time)
cuid: clt8fce80000609l695q79reh
slug: url-scrapings-in-an-lxc-with-node
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1709280696780/d4402d03-681e-4cdc-9e41-39ed3ecd3fe1.png
tags: ubuntu, software-development, javascript, development, nodejs, devops, lxc, containers, programming-ciovqvfcb008mb253jrczo9ye, web-scraping, tech-guide, tech-talk

---

# TL;DR.

This post is a guide to setting up a Linux Container (LXC) on Ubuntu. Within the container, I will run a Node.js script for scraping URLs. This piece covers the installation of LXD (LinuX Daemon), creating and configuring an LXC, and using NVM (Node Version Manager) to install Node.js and NPM. My tutorial also delves into web scraping with Axios and Cheerio, trawling the MariaDB website as a practical example. This article underscores the benefits of using containers for experimental development and isolated deployment, emphasizing the efficiency and flexibility it offers to coders.

> **Attributions:**
> 
> [https://ubuntu.com/tutorials/how-to-run-docker-inside-lxd-containers](https://ubuntu.com/tutorials/how-to-run-docker-inside-lxd-containers)***â†—.***

# An Introduction.

In this post, I show how to install the LinuX Daemon (LXD) on Ubuntu and set up a LinuX Container (LXC) for the purpose of running a Node.js script designed for URL scraping.

> The purpose of this post is to demonstrate the utility of LXCs by building a URL Scraper project.

# The Big Picture.

I use a number of isolation technologies including [LXC/LXD](https://solodev.app/installing-lxd-and-using-lxcs), [Distrobox](https://solodev.app/installing-distrobox), [Docker](https://solodev.app/installing-docker), and [Miniconda](https://solodev.app/installing-miniconda). I've already written about [the magic of VMs and containers](https://solodev.app/the-magic-of-vms-and-containers) and I find isolation technologies particularly useful for app deployment and coding experiments. Every developer needs to adopt isolation technologies for the sake of maintaining a stable host distro.

Also, I use an imaging solution called [CloneZilla](https://clonezilla.org/) that backs up my host system.

The faster I recover from disaster, the quicker I can get back to coding.

# Prerequisites.

* A Debian-based Linux distro (I use Ubuntu).
    

# Updating the System.

* I update my system:
    

```bash
sudo apt clean && \
sudo apt update && \
sudo apt dist-upgrade -y && \
sudo apt --fix-broken install && \
sudo apt autoclean && \
sudo apt autoremove -y
```

# What is LXD and LXC?

LXD (LinuX Daemon) is a container manager for creating and managing LXCs (LinuX Containers.) As a background service, LXD can automatically start containers when the host system boots.

LXCs (LinuX Containers) are isolated, OS-level virtualizations which, for efficiency, uses the Linux kernel of the host system. LXCs are virtual environments where its system processes can *not* affect other containers, or the host system, without running specific commands.

## Installing the LXD.

* I install the snap package manager, if required:
    

```bash
sudo apt install snapd -y
```

* I install the LXD:
    

```bash
sudo snap install lxd
```

* I initialise the LXD:
    

```bash
lxd init
```

> NOTE: I choose to use BTRFS and an existing host interface called eno1.  
> (I used the `ip addr` command to find the name of my host interface.)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1707522220201/f9686c89-4a0b-4749-9b59-32f1cc84bc7a.png align="center")

## Deleting the LXD.

* I can delete the LXD:
    

```bash
sudo snap remove --purge lxd
```

* I can also delete the LXD installer, if required:
    

```bash
sudo apt remove --purge lxd-installer
```

> NOTE: The `--purge` flag removes everything, including configuration files, etc.

## Setting Up an LXC.

* I list the existing containers:
    

```bash
lxc ls
```

* I launch a new container called Scrapings:
    

```bash
lxc launch ubuntu:22.04 Scrapings
```

* I bash into the container:
    

```bash
lxc exec Scrapings -- bash
```

* I update and upgrade the container:
    

```bash
sudo apt clean && \
sudo apt update && \
sudo apt dist-upgrade -y && \
sudo apt --fix-broken install && \
sudo apt autoclean && \
sudo apt autoremove -y
```

## Adding a User Account to the LXC.

* From within the container, I add a new user:
    

```bash
adduser yt
```

* I add the new user to the `sudo` group:
    

```bash
usermod -aG sudo yt
```

> NOTE: usermod let's me (-a)ppend the sudo (-G)roup to the yt account.

* I exit the container:
    

```bash
exit
```

> NOTE: I exit the `root` account to use the `yt` account.

## Setting the LXC Home Directory.

* From the terminal, I log in to the container with the `yt` account:
    

```bash
lxc exec Scrapings -- su yt
```

> NOTE: At the moment, the home directory is `/root`. This section will address the issue by changing the home directory to `~`.

* I use the Nano text editor to open the `.bashrc` file:
    

```bash
sudo nano ~/.bashrc
```

* I copy the following, add it (CTRL + SHIFT + V) to the bottom (CTRL + END) of the `.bashrc` file, save (CTRL + S) the changes, and exit (CTRL + X) Nano:
    

```bash
cd ~
```

## Hardening the LXC.

* From within the container, I use the `Nano` text editor to open the `sshd_config` file:
    

```bash
sudo nano /etc/ssh/sshd_config
```

* I copy the following, add it (CTRL + SHIFT + V) to the bottom (CTRL + END) of the `sshd_config` file, save (CTRL + S) the changes, and exit (CTRL + X) Nano:
    

```bash
PasswordAuthentication no
PermitRootLogin no
Protocol 2
Port = 2424
```

* I `restart` the `SSH` service:
    

```bash
sudo systemctl restart ssh
```

* I reboot the container:
    

```bash
sudo reboot
```

> NOTE: Within the container, I can also install (or enable) [UFW](https://solodev.app/creating-a-local-linux-container#heading-optional-enabling-and-setting-up-ufw), [Fail2Ban](https://solodev.app/creating-a-local-linux-container#heading-optional-installing-and-setting-up-fail2ban), and [CrowdSec](https://solodev.app/10-of-10-crowdsec-in-the-docker-container).

# What is Node.js, NPM, and NVM?

Node.js is a free, JavaScript (JS) server runtime. It runs JS code as single-threaded, non-blocking, asynchronous programs, which makes it very memory efficient. Node.js performs many system-level tasks, but is commonly used to run JS servers.

NPM (Node Package Manager) is the world's largest software registry. Open source developers use it to share packages with each other. Many organizations use NPM to manage private development as well. Most developers interact with NPM using the CLI (Command Line Interface), and usually ships with Node.js (which is a JavaScript server runtime).

NVM stands for Node.js Version Manager. It is used to switch between versions of Node.js (which is a JavaScript server runtime). NVM works on any POSIX-compliant shell (sh, dash, ksh, zsh, bash) and runs on Linux distros, macOS, and Windows WSL.

## Installing CURL.

* From the terminal, I log in to the container with the `yt` account:
    

```bash
lxc exec Scrapings -- su yt
```

> NOTE: The container is now pointing to its home directory.

* I install CURL:
    

```bash
sudo apt install -y curl
```

# Installing the NVM.

* I install the NVM (Node Version Manager):
    

```bash
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
```

> NOTE: The version numbers are [listed on the GitHub page](https://github.com/nvm-sh/nvm/tags).

* I activate the NVM:
    

```bash
source ~/.bashrc
```

# Using NVM to Install Node LTS and NPM.

* I use NVM to install the LTS version of Node:
    

```bash
nvm install --lts
```

> NOTE: The Node and NPM versions are listed on the screen.

# Making Node LTS the Default NVM Version.

* I make Node LTS the default NVM version, as listed in the previous section:
    

```bash
nvm alias default 20.11.0
```

# Confirming the Installations.

* I confirm the Node installation:
    

```bash
node -v
```

* I confirm the NPM installation:
    

```bash
npm -v
```

# What are URL Scrapings?

\[pending\]

## Installing the Requirements.

* I use NPM to install Axios and Cheerio:
    

```bash
npm install axios cheerio
```

## Creating the Scraper File.

* I use Nano to create the `scraper.js` JavaScript file:
    

```bash
sudo nano scraper.js
```

* I copy the following, paste (CTRL + SHIFT + V) it into the file, save (CTRL + S) the changes, and exit (CTRL + X) Nano:
    

```javascript
const fs = require('fs');
const axios = require('axios');
const cheerio = require('cheerio');

const scrapeLinks = async (url) => {
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  let counter = 0;

  $('a').each((index, element) => {
    setTimeout(() => {
      const link = $(element).attr('href');
      fs.appendFile('scraped.txt', 'https://mariadb.com/kb/en' + link + '\n', (err) => {
        if (err) throw err;
        counter++;
        console.log(`Link number ${counter} saved!`);
      });
    }, index * 1000);
  });
};

scrapeLinks('https://mariadb.com/kb/en');
```

> NOTE 1: There is a 1 second (1,000ms) pause between each request.

> NOTE 2: The MariaDB website is used as an example.

## Running the Scraper.

* At the terminal, I run `scraper.js`:
    

```bash
node scraper.js
```

## Collecting the Results.

* When `scraper.js` completes, I clear the terminal:
    

```bash
clear
```

* I display the results from `scraped.txt` in the terminal:
    

```bash
cat scraped.txt
```

* I use my mouse to select all the displayed links, and then CTRL + SHIFT + C to save everything to the clipboard.
    
* In a desktop editor like VS Code, I paste (CTRL + V) the results.
    

# The Results.

To explore URL scraping, I started with setting up an LXC on Ubuntu, installing Node.js and NPM using NVM, writing a scraper script in JavaScript, and using Node.js to scraping websites. Using Axios and Cheerio for scraping demonstrates the power and simplicity of using Node.js for web scraping tasks. This guide serves as a technical walk-through and also highlights the versatility of using containers for development tasks. It is evident that LXC containers offer a robust platform for developing and deploying many kinds of solutions, opening up a world of safe experimentation.

# In Conclusion.

In the realm of web development, efficiency and isolation are key. This is where Linux Containers (LXC) offers a lightweight alternative to traditional VMs. I've explored URL scraping within an LXC, leveraging the power of Node.js.

Why LXC? It provides an isolated environment for running applications, ensuring that your scraping activities don't interfere with your system or other containers.

Set up was a breeze:

* Starting with a fresh LXC on Ubuntu, I ventured into the installation of Node.js and NPM via NVM.
    
* The magic happened when I combined Axios and Cheerio for efficient web scraping, all running smoothly within my container.
    

The result? A streamlined process that not only simplifies development tasks but also opens up a sandbox for safe experimentation. Containers like LXC are invaluable tools for developers, offering a blend of flexibility, efficiency, and security.

Have you tried running development tasks within a container? What has your experience been like?

Let's discuss in the comments below!

Until next time: Be safe, be kind, be awesome.

#WebScraping #Nodejs #LXC #Development #TechTalk